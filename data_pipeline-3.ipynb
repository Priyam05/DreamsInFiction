{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.3"},"colab":{"name":"data_pipeline-3.ipynb","provenance":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"my8cRxrczdR5"},"source":["# Notebook Setup"]},{"cell_type":"code","metadata":{"id":"8nndhHv7zdSE"},"source":["from google.colab import drive\n","ROOT = \"/content/drive\"\n","print(ROOT)\n","drive.mount(ROOT)\n","\n","%cd /content/drive/My Drive/CompHumanities/Data/Notebooks"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fGbilxqBzdSE"},"source":["!pwd"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YdZT7j4IzdSF"},"source":["# Importing Libraries"]},{"cell_type":"code","metadata":{"id":"dNa-ljrNzdSF"},"source":["##NLTK##\n","import nltk\n","#nltk.download('punkt')\n","from nltk.corpus import stopwords\n","from nltk.tokenize import word_tokenize \n","\n","\n","##STOPWORDS##\n","import spacy\n","from gensim.parsing.preprocessing import remove_stopwords\n","# import en_core_web_sm\n","# nlp = en_core_web_sm.load()\n","\n","\n","##TRANSFORMERS##\n","from sklearn.pipeline import Pipeline\n","from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.feature_extraction.text import TfidfTransformer\n","from sklearn.naive_bayes import MultinomialNB\n","\n","##HATHI##\n","#!pip install htrc-feature-reader\n","import glob\n","import random\n","from htrc_features import FeatureReader, Volume, Page\n","\n","##WIKI##\n","#!pip install wikipedia\n","import wikipedia\n","import sys\n","from gensim.corpora import WikiCorpus\n","\n","##OTHERS##\n","import numpy as np\n","import pandas as pd\n","from os import listdir, path\n","from os.path import isfile, join\n","from sklearn.model_selection import train_test_split\n","from sklearn import linear_model\n","from scipy import sparse\n","import operator\n","from tqdm import tqdm\n","from collections import Counter\n","from nltk import FreqDist\n","random.seed(0)\n","np.random.seed(0)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XRl1mjGWzdSF"},"source":["from sklearn.model_selection import train_test_split"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"q7aXKHqyzdSG"},"source":["## Quick PKL IMPORTS"]},{"cell_type":"code","metadata":{"id":"-7wUifOuzdSG"},"source":["#FictionDF = pd.read_pickle('FictionDF.pkl')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HPuiIh_ZzdSG"},"source":["#NonFicDF = pd.read_pickle('NonFicDF.pkl')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"X51nrpd9zdSH"},"source":["# Data Pipeline"]},{"cell_type":"markdown","metadata":{"id":"TEx8xp6jzdSH"},"source":["## Hathi Data"]},{"cell_type":"markdown","metadata":{"id":"7SsDVDz3zdSH"},"source":["The data we'll use comes from the [HathiTrust Extracted Features dataset](https://analytics.hathitrust.org/datasets).\n","\n","We've sampled **SPECIFY SIZE of fiction>** from Ted Underwood's [metadata](https://github.com/tedunderwood/hathimetadata). This fiction data will act as the non-dream data"]},{"cell_type":"code","metadata":{"id":"anbMW06MzdSI"},"source":["def get_feature_reader(path):\n","    paths = glob.glob(path+\"/*bz2\", recursive=True)\n","    return FeatureReader(paths)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"oODyoF0HzdSI"},"source":["def get_data(vols, label):\n","    \n","    df = []\n","    \n","    for vol in tqdm(vols.volumes(), total=500):        \n","        chunks = vol.tokenlist(chunk=True, chunk_target=250)\n","        chunks = chunks.reset_index()\n","        df.append(chunks)\n","    df = pd.concat(df)\n","    \n","    return df"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TpmPCTJ0zdSI"},"source":["**Fiction Data** , label = 1"]},{"cell_type":"code","metadata":{"id":"SsofaxIizdSJ","outputId":"204a671a-2cbd-483c-f1e5-15a7eb9bcf74"},"source":["FictionDF = get_data(get_feature_reader(\"data/fiction\"),1)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["100%|██████████| 500/500 [02:43<00:00,  3.05it/s]\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"1SenfRkkzdSK","outputId":"24114966-38c5-4bb5-a089-87743c0b9a27"},"source":["FictionDF = FictionDF.drop(columns='section').rename(columns={'count':'counts'})\n","FictionDF.head()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>chunk</th>\n","      <th>token</th>\n","      <th>pos</th>\n","      <th>counts</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","      <td>''</td>\n","      <td>''</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>,</td>\n","      <td>,</td>\n","      <td>17</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>1</td>\n","      <td>.</td>\n","      <td>.</td>\n","      <td>8</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>1</td>\n","      <td>.</td>\n","      <td>UNK</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>1</td>\n","      <td>1989</td>\n","      <td>CD</td>\n","      <td>3</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   chunk token  pos  counts\n","0      1    ''   ''       1\n","1      1     ,    ,      17\n","2      1     .    .       8\n","3      1     .  UNK       2\n","4      1  1989   CD       3"]},"metadata":{"tags":[]},"execution_count":49}]},{"cell_type":"code","metadata":{"id":"lpe1_50ezdSL"},"source":["#Only pick those chunks which are having less than or equal to 250 tokens\n","FictionDFGROUP  = FictionDF.groupby('chunk').count()\n","FictionDFGROUP = FictionDFGROUP[FictionDFGROUP['token']<=250]\n","FictionDFGROUP = FictionDFGROUP.reset_index()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YLSF2Dp9zdSL"},"source":["#Randomly select 300 chunks for our sample\n","NumChunks = 300 if len(FictionDFGROUP) > 300 else len(FictionDFGROUP)\n","randChunks = random.sample(list(FictionDFGROUP['chunk'].unique()), NumChunks)\n","FicDFSample = FictionDF[FictionDF.chunk.isin(randChunks)]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"I_MKxBpYzdSM","outputId":"14558f3d-6ec9-4e01-e10f-3f77f5e1d6a4"},"source":["#Sanity Check\n","FicDFSample.groupby('chunk').count()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>token</th>\n","      <th>pos</th>\n","      <th>counts</th>\n","    </tr>\n","    <tr>\n","      <th>chunk</th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>744</th>\n","      <td>241</td>\n","      <td>241</td>\n","      <td>241</td>\n","    </tr>\n","    <tr>\n","      <th>758</th>\n","      <td>189</td>\n","      <td>189</td>\n","      <td>189</td>\n","    </tr>\n","    <tr>\n","      <th>761</th>\n","      <td>170</td>\n","      <td>170</td>\n","      <td>170</td>\n","    </tr>\n","    <tr>\n","      <th>763</th>\n","      <td>178</td>\n","      <td>178</td>\n","      <td>178</td>\n","    </tr>\n","    <tr>\n","      <th>764</th>\n","      <td>180</td>\n","      <td>180</td>\n","      <td>180</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>1385</th>\n","      <td>192</td>\n","      <td>192</td>\n","      <td>192</td>\n","    </tr>\n","    <tr>\n","      <th>1386</th>\n","      <td>185</td>\n","      <td>185</td>\n","      <td>185</td>\n","    </tr>\n","    <tr>\n","      <th>1390</th>\n","      <td>202</td>\n","      <td>202</td>\n","      <td>202</td>\n","    </tr>\n","    <tr>\n","      <th>1392</th>\n","      <td>191</td>\n","      <td>191</td>\n","      <td>191</td>\n","    </tr>\n","    <tr>\n","      <th>1398</th>\n","      <td>198</td>\n","      <td>198</td>\n","      <td>198</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>300 rows × 3 columns</p>\n","</div>"],"text/plain":["       token  pos  counts\n","chunk                    \n","744      241  241     241\n","758      189  189     189\n","761      170  170     170\n","763      178  178     178\n","764      180  180     180\n","...      ...  ...     ...\n","1385     192  192     192\n","1386     185  185     185\n","1390     202  202     202\n","1392     191  191     191\n","1398     198  198     198\n","\n","[300 rows x 3 columns]"]},"metadata":{"tags":[]},"execution_count":181}]},{"cell_type":"code","metadata":{"id":"KKpC7ZlBzdSM"},"source":["FictionDF.to_pickle('FictionDF.pkl')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GCqefoQXzdSM"},"source":["FicDFSample.to_pickle('FicDFSample.pkl')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QNu5Tr31zdSN"},"source":["**Non Fiction Data** , label = 0"]},{"cell_type":"code","metadata":{"id":"Ty-CCHufzdSN","outputId":"236e4eaf-451b-4739-d46f-c4efba05ec65"},"source":["NonFicDF = get_data(get_feature_reader(\"data/nonfiction\"),0)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["100%|██████████| 500/500 [03:17<00:00,  2.53it/s]\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"t63CBmvQzdSN","outputId":"68b715c9-6ed9-4766-cce8-2648a91a4310"},"source":["NonFicDF = NonFicDF.drop(columns='section').rename(columns={'count':'counts'})\n","NonFicDF.head()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>chunk</th>\n","      <th>token</th>\n","      <th>pos</th>\n","      <th>counts</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","      <td>\".:***</td>\n","      <td>UNK</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>'</td>\n","      <td>''</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>1</td>\n","      <td>'</td>\n","      <td>POS</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>1</td>\n","      <td>,</td>\n","      <td>,</td>\n","      <td>13</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>1</td>\n","      <td>-</td>\n","      <td>:</td>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   chunk   token  pos  counts\n","0      1  \".:***  UNK       1\n","1      1       '   ''       2\n","2      1       '  POS       1\n","3      1       ,    ,      13\n","4      1       -    :       1"]},"metadata":{"tags":[]},"execution_count":56}]},{"cell_type":"code","metadata":{"id":"Ch36WrZfzdSO"},"source":["#Only pick those chunks which are having less than or equal to 250 tokens\n","NonFicDFGROUP  = NonFicDF.groupby('chunk').count()\n","NonFicDFGROUP = NonFicDFGROUP[NonFicDFGROUP['token']<=250]\n","NonFicDFGROUP = NonFicDFGROUP.reset_index()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JeQ-xcO4zdSO"},"source":["#Randomly select 300 chunks for our sample\n","NumChunks = 300 if len(NonFicDFGROUP) > 300 else len(NonFicDFGROUP)\n","randChunks = random.sample(list(NonFicDFGROUP['chunk'].unique()), NumChunks)\n","NonFicDFSample = NonFicDF[NonFicDF.chunk.isin(randChunks)]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uwHSNwZdzdSO","outputId":"22ea866f-5635-41c0-844b-89fece73d1da"},"source":["#Sanity Check\n","NonFicDFSample.groupby('chunk').count()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>token</th>\n","      <th>pos</th>\n","      <th>counts</th>\n","    </tr>\n","    <tr>\n","      <th>chunk</th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>1043</th>\n","      <td>118</td>\n","      <td>118</td>\n","      <td>118</td>\n","    </tr>\n","    <tr>\n","      <th>1044</th>\n","      <td>206</td>\n","      <td>206</td>\n","      <td>206</td>\n","    </tr>\n","    <tr>\n","      <th>1045</th>\n","      <td>231</td>\n","      <td>231</td>\n","      <td>231</td>\n","    </tr>\n","    <tr>\n","      <th>1046</th>\n","      <td>245</td>\n","      <td>245</td>\n","      <td>245</td>\n","    </tr>\n","    <tr>\n","      <th>1048</th>\n","      <td>250</td>\n","      <td>250</td>\n","      <td>250</td>\n","    </tr>\n","    <tr>\n","      <th>1051</th>\n","      <td>222</td>\n","      <td>222</td>\n","      <td>222</td>\n","    </tr>\n","    <tr>\n","      <th>1054</th>\n","      <td>243</td>\n","      <td>243</td>\n","      <td>243</td>\n","    </tr>\n","    <tr>\n","      <th>1055</th>\n","      <td>182</td>\n","      <td>182</td>\n","      <td>182</td>\n","    </tr>\n","    <tr>\n","      <th>1056</th>\n","      <td>190</td>\n","      <td>190</td>\n","      <td>190</td>\n","    </tr>\n","    <tr>\n","      <th>1057</th>\n","      <td>207</td>\n","      <td>207</td>\n","      <td>207</td>\n","    </tr>\n","    <tr>\n","      <th>1058</th>\n","      <td>196</td>\n","      <td>196</td>\n","      <td>196</td>\n","    </tr>\n","    <tr>\n","      <th>1059</th>\n","      <td>182</td>\n","      <td>182</td>\n","      <td>182</td>\n","    </tr>\n","    <tr>\n","      <th>1060</th>\n","      <td>185</td>\n","      <td>185</td>\n","      <td>185</td>\n","    </tr>\n","    <tr>\n","      <th>1061</th>\n","      <td>214</td>\n","      <td>214</td>\n","      <td>214</td>\n","    </tr>\n","    <tr>\n","      <th>1062</th>\n","      <td>169</td>\n","      <td>169</td>\n","      <td>169</td>\n","    </tr>\n","    <tr>\n","      <th>1063</th>\n","      <td>200</td>\n","      <td>200</td>\n","      <td>200</td>\n","    </tr>\n","    <tr>\n","      <th>1064</th>\n","      <td>64</td>\n","      <td>64</td>\n","      <td>64</td>\n","    </tr>\n","    <tr>\n","      <th>1121</th>\n","      <td>4</td>\n","      <td>4</td>\n","      <td>4</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["       token  pos  counts\n","chunk                    \n","1043     118  118     118\n","1044     206  206     206\n","1045     231  231     231\n","1046     245  245     245\n","1048     250  250     250\n","1051     222  222     222\n","1054     243  243     243\n","1055     182  182     182\n","1056     190  190     190\n","1057     207  207     207\n","1058     196  196     196\n","1059     182  182     182\n","1060     185  185     185\n","1061     214  214     214\n","1062     169  169     169\n","1063     200  200     200\n","1064      64   64      64\n","1121       4    4       4"]},"metadata":{"tags":[]},"execution_count":197}]},{"cell_type":"code","metadata":{"id":"Wd3oYOSlzdSP"},"source":["NonFicDF.to_pickle('NonFicDF.pkl')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"q-k1AdFmzdSP"},"source":["NonFicDFSample.to_pickle('NonFicDFSample.pkl')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Kypri0pVzdSP"},"source":["## DreamBank Data"]},{"cell_type":"code","metadata":{"id":"GqETyBg0zdSP"},"source":["dreams = pd.read_csv(\"DreamBank.csv\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jB8tYSsezdSP"},"source":["dreams['split'] = dreams['content'].apply(lambda x : nltk.word_tokenize(x.lower()))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6ddF26-qzdSQ"},"source":["dreams['counts'] =dreams.split.apply(lambda x: Counter(x))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DiQDuiI8zdSQ"},"source":["dreams = dreams.loc[:,['split','counts']]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"eh2HYh3uzdSR"},"source":["dreamsDF = pd.DataFrame.from_records(dreams.counts.values.tolist()).stack().reset_index().rename(columns={'level_0':'chunk','level_1':'tokens',0:'counts'})"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"biRfbcKCzdSS"},"source":["dreamsDF['pos'] = dreamsDF['tokens'].apply(lambda x : (nltk.pos_tag([x]))[0][1] )"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"scrolled":true,"id":"47V9YqABzdST"},"source":["#Only pick those chunks which are having less than or equal to 250 tokens\n","dreamsDFGROUP  = dreamsDF.groupby('chunk').count()\n","dreamsDFGROUP = dreamsDFGROUP[dreamsDFGROUP['tokens']<=250]\n","dreamsDFGROUP = dreamsDFGROUP.reset_index()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8rMQ6HIuzdST"},"source":["#Randomly select 300 chunks for our sample\n","NumChunks = 300 if len(dreamsDFGROUP) > 300 else len(dreamsDFGROUP)\n","randChunks = random.sample(list(dreamsDFGROUP['chunk'].unique()), NumChunks)\n","dreamsDFSample = dreamsDF[dreamsDF.chunk.isin(randChunks)]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"enISgllwzdST","outputId":"d89703dd-58b6-4875-e99f-51bc03a81e29"},"source":["#Sanity Check\n","dreamsDFSample.groupby('chunk').count()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>tokens</th>\n","      <th>counts</th>\n","      <th>pos</th>\n","    </tr>\n","    <tr>\n","      <th>chunk</th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>44</th>\n","      <td>73</td>\n","      <td>73</td>\n","      <td>73</td>\n","    </tr>\n","    <tr>\n","      <th>46</th>\n","      <td>177</td>\n","      <td>177</td>\n","      <td>177</td>\n","    </tr>\n","    <tr>\n","      <th>78</th>\n","      <td>74</td>\n","      <td>74</td>\n","      <td>74</td>\n","    </tr>\n","    <tr>\n","      <th>176</th>\n","      <td>149</td>\n","      <td>149</td>\n","      <td>149</td>\n","    </tr>\n","    <tr>\n","      <th>237</th>\n","      <td>56</td>\n","      <td>56</td>\n","      <td>56</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>25966</th>\n","      <td>33</td>\n","      <td>33</td>\n","      <td>33</td>\n","    </tr>\n","    <tr>\n","      <th>26104</th>\n","      <td>144</td>\n","      <td>144</td>\n","      <td>144</td>\n","    </tr>\n","    <tr>\n","      <th>26184</th>\n","      <td>124</td>\n","      <td>124</td>\n","      <td>124</td>\n","    </tr>\n","    <tr>\n","      <th>26262</th>\n","      <td>136</td>\n","      <td>136</td>\n","      <td>136</td>\n","    </tr>\n","    <tr>\n","      <th>26305</th>\n","      <td>195</td>\n","      <td>195</td>\n","      <td>195</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>300 rows × 3 columns</p>\n","</div>"],"text/plain":["       tokens  counts  pos\n","chunk                     \n","44         73      73   73\n","46        177     177  177\n","78         74      74   74\n","176       149     149  149\n","237        56      56   56\n","...       ...     ...  ...\n","25966      33      33   33\n","26104     144     144  144\n","26184     124     124  124\n","26262     136     136  136\n","26305     195     195  195\n","\n","[300 rows x 3 columns]"]},"metadata":{"tags":[]},"execution_count":204}]},{"cell_type":"code","metadata":{"id":"UeoL8WG8zdST"},"source":["#dreamsDF = pd.read_pickle('dreamsDF.pkl')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GqusdGMmzdSU"},"source":["#dreamsDF.to_pickle('dreamsDF.pkl')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZcEd0MlozdSU"},"source":["dreamsDFSample.to_pickle('dreamsDFSample.pkl')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-GGCrHRMzdSU"},"source":["## Wiki Data"]},{"cell_type":"code","metadata":{"id":"6yV7A0GFzdSU"},"source":["# def make_corpus(in_f, out_f):\n","\n","#     \"\"\"Convert Wikipedia xml dump file to text corpus\"\"\"\n","\n","#     output = open(out_f, 'w')\n","#     wiki = WikiCorpus(in_f)\n","\n","#     i = 0\n","#     for text in wiki.get_texts():\n","#         output.write(bytes(' '.join(text), 'utf-8').decode('utf-8') + '\\n')\n","#         i = i + 1\n","#         if (i % 10000 == 0):\n","#             print('Processed ' + str(i) + ' articles')\n","#     output.close()\n","#     print('Processing complete!')\n","\n","##Reading corpus##\n","\n","# in_f = \"enwiki-latest-pages-articles.xml.bz2\"\n","# out_f = \"wiki_en.txt\"\n","# make_corpus(in_f, out_f)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"t3ZguUanzdSV"},"source":["wiki = pd.read_csv(\"wiki_en.txt\", header = None).rename(columns={0:'content'})"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Su5cHRMlzdSV"},"source":["wiki['content'] = wiki.content.apply(lambda x: [' '.join(x.split()[i:i+n]) for i in range(0,len(x.split()),250)])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TBZhMA0DzdSW"},"source":["wiki = wiki.explode('content')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8TLxnBoDzdSW"},"source":["wiki['split'] = wiki['content'].apply(lambda x : nltk.word_tokenize(x.lower()))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5pzr0dHqzdSW"},"source":["wiki['counts'] = wiki.split.apply(lambda x: Counter(x))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"EkXYiZqpzdSW"},"source":["wiki = wiki.loc[:,['split','counts']]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CQX8bKZLzdSW"},"source":["wikiDF = pd.DataFrame.from_records(wiki.counts.values.tolist()).stack().reset_index().rename(columns={'level_0':'chunk','level_1':'tokens',0:'counts'})"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ULeWXw_MzdSW"},"source":["wikiDF['pos'] = wikiDF['tokens'].apply(lambda x : (nltk.pos_tag([x]))[0][1] )"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vwj4xpIlzdSW","outputId":"a795f717-9cb5-48c3-fcfa-3fa28c8fec27"},"source":["wikiDF.head()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>chunk</th>\n","      <th>tokens</th>\n","      <th>counts</th>\n","      <th>pos</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>anarchism</td>\n","      <td>5.0</td>\n","      <td>NN</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0</td>\n","      <td>is</td>\n","      <td>4.0</td>\n","      <td>VBZ</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0</td>\n","      <td>political</td>\n","      <td>2.0</td>\n","      <td>JJ</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0</td>\n","      <td>philosophy</td>\n","      <td>1.0</td>\n","      <td>NN</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0</td>\n","      <td>and</td>\n","      <td>10.0</td>\n","      <td>CC</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   chunk      tokens  counts  pos\n","0      0   anarchism     5.0   NN\n","1      0          is     4.0  VBZ\n","2      0   political     2.0   JJ\n","3      0  philosophy     1.0   NN\n","4      0         and    10.0   CC"]},"metadata":{"tags":[]},"execution_count":108}]},{"cell_type":"code","metadata":{"id":"Imsb3IX4zdSX"},"source":["wikiDF.to_pickle('wikiDF.pkl')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rSU3RuOgzdSX"},"source":["#Only pick those chunks which are having less than or equal to 250 tokens\n","wikiDFGROUP  = wikiDF.groupby('chunk').count()\n","wikiDFGROUP = wikiDFGROUP[wikiDFGROUP['tokens']<=250]\n","wikiDFGROUP = wikiDFGROUP.reset_index()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UdPzMclPzdSX"},"source":["#Randomly select 300 chunks for our sample\n","NumChunks = 300 if len(wikiDFGROUP) > 300 else len(wikiDFGROUP)\n","randChunks = random.sample(list(wikiDFGROUP['chunk'].unique()), NumChunks)\n","wikiDFSample = wikiDF[wikiDF.chunk.isin(randChunks)]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LZjpBysVzdSX","outputId":"fdb22330-26b3-4f64-e4df-1de3a3869b9b"},"source":["#Sanity Check\n","wikiDFSample.groupby('chunk').count()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>tokens</th>\n","      <th>counts</th>\n","      <th>pos</th>\n","    </tr>\n","    <tr>\n","      <th>chunk</th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>24</th>\n","      <td>151</td>\n","      <td>151</td>\n","      <td>151</td>\n","    </tr>\n","    <tr>\n","      <th>116</th>\n","      <td>130</td>\n","      <td>130</td>\n","      <td>130</td>\n","    </tr>\n","    <tr>\n","      <th>298</th>\n","      <td>151</td>\n","      <td>151</td>\n","      <td>151</td>\n","    </tr>\n","    <tr>\n","      <th>353</th>\n","      <td>156</td>\n","      <td>156</td>\n","      <td>156</td>\n","    </tr>\n","    <tr>\n","      <th>381</th>\n","      <td>147</td>\n","      <td>147</td>\n","      <td>147</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>11932</th>\n","      <td>165</td>\n","      <td>165</td>\n","      <td>165</td>\n","    </tr>\n","    <tr>\n","      <th>11946</th>\n","      <td>147</td>\n","      <td>147</td>\n","      <td>147</td>\n","    </tr>\n","    <tr>\n","      <th>11976</th>\n","      <td>154</td>\n","      <td>154</td>\n","      <td>154</td>\n","    </tr>\n","    <tr>\n","      <th>12010</th>\n","      <td>149</td>\n","      <td>149</td>\n","      <td>149</td>\n","    </tr>\n","    <tr>\n","      <th>12052</th>\n","      <td>159</td>\n","      <td>159</td>\n","      <td>159</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>300 rows × 3 columns</p>\n","</div>"],"text/plain":["       tokens  counts  pos\n","chunk                     \n","24        151     151  151\n","116       130     130  130\n","298       151     151  151\n","353       156     156  156\n","381       147     147  147\n","...       ...     ...  ...\n","11932     165     165  165\n","11946     147     147  147\n","11976     154     154  154\n","12010     149     149  149\n","12052     159     159  159\n","\n","[300 rows x 3 columns]"]},"metadata":{"tags":[]},"execution_count":209}]},{"cell_type":"code","metadata":{"id":"FTiMHCwOzdSX"},"source":["wikiDFSample.to_pickle('wikiDFSample.pkl')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"085Yg8xqzdSY"},"source":["# REMOVING STOPWORDS"]},{"cell_type":"code","metadata":{"id":"OiHKFNCkzdSY"},"source":["def stopword_gensim(text):\n","    filtered_sentence = remove_stopwords(text)\n","    return(filtered_sentence)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jiOaE0IdzdSY"},"source":["# PIPELINE"]},{"cell_type":"code","metadata":{"id":"lNV3WqdXzdSY"},"source":["dreamsDF.content = dreamsDF.content.apply(lambda x: stopword_gensim(x))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Vd43rerizdSY","outputId":"bc542ab1-451c-461a-a13d-37e9913083b7"},"source":["text_clf = Pipeline(\n","    [(  'vect', CountVectorizer()),\n","        ('tfidf', TfidfTransformer()),\n","        ('clf', MultinomialNB()\n","                     )])\n","\n","text_clf = text_clf.fit(dreamsDF.content, dreamsDF.target)\n","text_clf"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Pipeline(steps=[('vect', CountVectorizer()), ('tfidf', TfidfTransformer()),\n","                ('clf', MultinomialNB())])"]},"metadata":{"tags":[]},"execution_count":125}]}]}